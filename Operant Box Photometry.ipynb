{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c77d5f",
   "metadata": {},
   "source": [
    "Used for analyzing photometry data collected using Synapse software (Tucker Davis) with TTLs to timestamp events sent from MedAssociates operant boxes.\n",
    "\n",
    "Code for curve fit and motion correction adapted from Simpson et al. 2024 (https://doi.org/10.1016/j.neuron.2023.11.016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import tdt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import os\n",
    "import pylab as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a1875",
   "metadata": {},
   "source": [
    "### Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e184bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each mouse to a group\n",
    "group_id = {\n",
    "    'mouseID':'groupname',\n",
    "    'mouseID':'groupname'\n",
    "    }\n",
    "\n",
    "group_ids = list(set(group_id.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder containing 1 day of TDT data from multiple animals\n",
    "data_directory = './Data'\n",
    "\n",
    "#Specify where to save the output\n",
    "data_output = './Output'\n",
    "\n",
    "# Get a list of all folders in the 'data' directory\n",
    "folderlist = [item for item in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, item))]\n",
    "\n",
    "# Extract metadata from the folder names and read in the data. Assumes folder names contain eartags as '1234-5678'\n",
    "#Creates 'alldat', a list of structs containing all the data \n",
    "masterdat = []\n",
    "for foldername in folderlist:\n",
    "    dat = {}\n",
    "    \n",
    "    if len(foldername) == 18:\n",
    "        dat['mouse1'] = foldername[0:4]\n",
    "        dat['mouse2'] = '0000'\n",
    "        \n",
    "        dat['date'] = foldername[5:11]\n",
    "        \n",
    "    else:    \n",
    "        dat['mouse1'] = foldername[0:4]\n",
    "        dat['mouse2'] = foldername[5:9]\n",
    "    \n",
    "        dat['date'] = foldername[10:16]\n",
    "\n",
    "    dat['blockpath'] = foldername  \n",
    "\n",
    "    # Reading in data\n",
    "    dat['data'] = tdt.read_block(os.path.join(data_directory, dat['blockpath']))\n",
    "    \n",
    "    masterdat.append(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d09a4",
   "metadata": {},
   "source": [
    "### Pulling data and TTLs for each mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a381e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \"_470A\" etc. values and epocs may need to be changed if recording from a different photometry setup\n",
    "\n",
    "alldat = []\n",
    "\n",
    "for f in range(len(masterdat)):\n",
    "    \n",
    "    if masterdat[f]['mouse1'] != '0000':\n",
    "        dat1 = {}\n",
    "        dat1['mouseID'] = masterdat[f]['mouse1']\n",
    "        dat1['green'] = masterdat[f]['data'].streams._470A.data\n",
    "        dat1['isos'] = masterdat[f]['data'].streams._405A.data\n",
    "        dat1['TTL_values'] = masterdat[f]['data'].epocs.BOX1.data\n",
    "        dat1['TTL_timestamps'] = masterdat[f]['data'].epocs.BOX1.onset\n",
    "        dat1['sampling_rate'] = masterdat[f]['data'].streams._470A.fs\n",
    "        alldat.append(dat1)\n",
    "    \n",
    "    if masterdat[f]['mouse2'] != '0000':\n",
    "        dat2 = {}\n",
    "        dat2['mouseID'] = masterdat[f]['mouse2']\n",
    "        dat2['green'] = masterdat[f]['data'].streams._470B.data\n",
    "        dat2['isos'] = masterdat[f]['data'].streams._405B.data\n",
    "        dat2['TTL_values'] = masterdat[f]['data'].epocs.BOX2.data\n",
    "        dat2['TTL_timestamps'] = masterdat[f]['data'].epocs.BOX2.onset \n",
    "        dat2['sampling_rate'] = masterdat[f]['data'].streams._470B.fs\n",
    "        alldat.append(dat2)\n",
    "\n",
    "#assigning group name\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    alldat[f]['group'] = group_id[alldat[f]['mouseID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim so length matches\n",
    "    \n",
    "for f in range(len(alldat)):\n",
    "    \n",
    "    a = len(alldat[f]['green'])\n",
    "    b = len(alldat[f]['isos'])\n",
    "    if b < a:\n",
    "        alldat[f]['green'] = alldat[f]['green'][0:b]\n",
    "    if a < b:\n",
    "        alldat[f]['isos'] = alldat[f]['isos'][0:a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time series in seconds\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    alldat[f]['time'] = (np.arange(1,len(alldat[f]['green'])+1))/alldat[f]['sampling_rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db03d1",
   "metadata": {},
   "source": [
    "### Remove artifact at start and downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7927f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove artifact at start\n",
    "    \n",
    "t = 8 # time threshold (in seconds) below which we will discard\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    indA = np.argmax(alldat[f]['time'] > t) # find first index of when time crosses threshold\n",
    "    alldat[f]['time'] = alldat[f]['time'][indA:] # reformat vector to only include allowed time\n",
    "    alldat[f]['trimmed_green'] = alldat[f]['green'][indA:]\n",
    "    alldat[f]['trimmed_isos'] = alldat[f]['isos'][indA:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b483097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last 10 seconds from the end of each recording in case you forgot to stop before unplugging\n",
    "\n",
    "end_trim = 10  # Time in seconds to remove from the end of each recording\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    # Get the total duration of the recording\n",
    "    T = alldat[f]['time'][-1]\n",
    "    \n",
    "    # Define the new end threshold\n",
    "    t_end = T - end_trim\n",
    "    \n",
    "    # Find the first index where time exceeds t_end\n",
    "    indB = np.argmax(alldat[f]['time'] > t_end)\n",
    "    \n",
    "    # Edge case: if the last time is still below or equal to t_end, it means there's effectively nothing to remove.\n",
    "    if indB == 0 and alldat[f]['time'][-1] <= t_end:\n",
    "        continue\n",
    "    else:\n",
    "        # Trim up to (not including) indB\n",
    "        alldat[f]['time'] = alldat[f]['time'][:indB]\n",
    "        alldat[f]['trimmed_green'] = alldat[f]['trimmed_green'][:indB]\n",
    "        alldat[f]['trimmed_isos'] = alldat[f]['trimmed_isos'][:indB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20267c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample and average \n",
    "\n",
    "N = 100 # Average every N samples into 1 value\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    F_green=[]\n",
    "    for i in range(0, len(alldat[f]['trimmed_green']), N):\n",
    "        small_list = np.mean(alldat[f]['trimmed_green'][i:i+N-1])\n",
    "        F_green.append(small_list)\n",
    "    alldat[f]['downsampled_green'] = np.array(F_green)\n",
    "    \n",
    "\n",
    "    F_isos=[]\n",
    "    for i in range(0, len(alldat[f]['trimmed_isos']), N):\n",
    "        small_lst = np.mean(alldat[f]['trimmed_isos'][i:i+N-1])\n",
    "        F_isos.append(small_lst)\n",
    "    alldat[f]['downsampled_isos'] = np.array(F_isos)\n",
    "    \n",
    "    alldat[f]['downsampled_time'] = alldat[f]['time'][::N]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0c243",
   "metadata": {},
   "source": [
    "### Fit double exponential to each curve and use to correct for bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The double exponential curve we are going to fit.\n",
    "def double_exponential(t, const, amp_fast, amp_slow, tau_slow, tau_multiplier):\n",
    "    '''Compute a double exponential function with constant offset.\n",
    "    Parameters:\n",
    "    t       : Time vector in seconds.\n",
    "    const   : Amplitude of the constant offset. \n",
    "    amp_fast: Amplitude of the fast component.  \n",
    "    amp_slow: Amplitude of the slow component.  \n",
    "    tau_slow: Time constant of slow component in seconds.\n",
    "    tau_multiplier: Time constant of fast component relative to slow. \n",
    "    '''\n",
    "    tau_fast = tau_slow*tau_multiplier\n",
    "    return const+amp_slow*np.exp(-t/tau_slow)+amp_fast*np.exp(-t/tau_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2a38c-3dcd-4ecb-916a-4bbff0ba3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(trace, tau_slow_min=0.0001, tau_slow_max=30):\n",
    "\n",
    "    signal = trace\n",
    "    timecourse = time\n",
    "\n",
    "    # Amplitude bounds\n",
    "    amp_min = 0  # Assuming amplitude cannot be negative\n",
    "    amp_max = 2 * np.max(signal)  # Allowing for some flexibility\n",
    "\n",
    "    # Time constant bounds based on the duration of the experiment\n",
    "    time_constant_min = tau_slow_min * (timecourse[-1] - timecourse[0])  \n",
    "    time_constant_max = tau_slow_max * (timecourse[-1] - timecourse[0])  \n",
    "\n",
    "    # Offset bounds\n",
    "    offset_min = np.min(signal) if np.min(signal) < 0 else 0  \n",
    "    offset_max = np.max(signal)\n",
    "\n",
    "    return (\n",
    "        [amp_min, amp_min, amp_min, time_constant_min, offset_min],\n",
    "        [amp_max, amp_max, amp_max, time_constant_max, offset_max]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f605c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in range(len(alldat)):\n",
    "\n",
    "# Assign the downsampled curves to variable names. \n",
    "    \n",
    "    green=alldat[f]['downsampled_green']\n",
    "    isos=alldat[f]['downsampled_isos']\n",
    "    time=alldat[f]['downsampled_time']\n",
    "\n",
    "    # Fit curve to green signal.\n",
    "    max_sig = np.max(green)\n",
    "    inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "    bounds = get_bounds(green)\n",
    "    green_parms, parm_cov = curve_fit(double_exponential, time, green, \n",
    "                                    p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "    green_expfit = double_exponential(time, *green_parms)\n",
    "    alldat[f]['expfitgreen'] = green_expfit\n",
    "\n",
    "    # Fit curve to 405 signal.\n",
    "    max_sig = np.max(isos)\n",
    "    inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "    bounds = get_bounds(isos)\n",
    "    isos_parms, parm_cov = curve_fit(double_exponential, time, isos, \n",
    "                                        p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "    isos_expfit = double_exponential(time, *isos_parms)\n",
    "    alldat[f]['expfitisos'] = isos_expfit\n",
    "\n",
    "    #plot fits over denoised data\n",
    "    fig,ax1=plt.subplots()  \n",
    "    plot1=ax1.plot(time, green, 'g', label='green')\n",
    "    plot3=ax1.plot(time, green_expfit, color='k', linewidth=1.5, label='Exponential fit') \n",
    "    ax2=plt.twinx()\n",
    "    plot2=ax2.plot(time, isos, color='b', label='isos') \n",
    "    plot4=ax2.plot(time, isos_expfit,color='k', linewidth=1.5) \n",
    "\n",
    "\n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.set_ylabel('Green Signal (V)', color='g')\n",
    "    ax2.set_ylabel('Isos Signal (V)', color='b')\n",
    "    ax1.set_title(alldat[f]['mouseID'])\n",
    "\n",
    "    lines = plot1 + plot2 + plot3\n",
    "    labels = [l.get_label() for l in lines]  \n",
    "    legend = ax1.legend(lines, labels, loc='upper right'); \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e58fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Subtract curve to correct for bleaching\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    green_detrended = alldat[f]['downsampled_green'] - alldat[f]['expfitgreen']\n",
    "    alldat[f]['detrended_green'] = green_detrended\n",
    "    isos_detrended = alldat[f]['downsampled_isos'] - alldat[f]['expfitisos']\n",
    "    alldat[f]['detrended_isos'] = isos_detrended\n",
    "\n",
    "    time=alldat[f]['downsampled_time']\n",
    "\n",
    "    fig,ax1=plt.subplots()  \n",
    "    plot1=ax1.plot(time, green_detrended, 'g', label='green')\n",
    "    ax2=plt.twinx()\n",
    "    plot2=ax2.plot(time, isos_detrended, color='b', label='isos') \n",
    "\n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.set_ylabel('Green Signal (V)', color='g')\n",
    "    ax2.set_ylabel('Isos Signal (V)', color='b')\n",
    "    ax1.set_title(alldat[f]['mouseID'])\n",
    "\n",
    "    lines = plot1+plot2 \n",
    "    labels = [l.get_label() for l in lines]  \n",
    "    legend = ax1.legend(lines, labels, loc='upper right'); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ff033",
   "metadata": {},
   "source": [
    "### Correcting for movement artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd2920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in range(len(alldat)):\n",
    "    \n",
    "    #Scatter plot of Isos vs Green signal\n",
    "    \n",
    "    green_detrended = alldat[f]['detrended_green']\n",
    "    isos_detrended = alldat[f]['detrended_isos']\n",
    "    time = alldat[f]['downsampled_time']\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x=isos_detrended, y=green_detrended)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(isos_detrended[::5], green_detrended[::5],alpha=0.1, marker='.')\n",
    "    x = np.array(plt.xlim())\n",
    "    plt.plot(x, intercept+slope*x)\n",
    "    plt.xlabel('Isos')\n",
    "    plt.ylabel('Green')\n",
    "    plt.title('Isos - Green correlation')\n",
    "\n",
    "    print('Slope    : {:.3f}'.format(slope))\n",
    "    print('R-squared: {:.3f}'.format(r_value**2))\n",
    "\n",
    "    #Calculate motion effect\n",
    "\n",
    "    green_est_motion = intercept + slope * isos_detrended\n",
    "    green_corrected = green_detrended - green_est_motion\n",
    "\n",
    "    alldat[f]['corrected_green']=green_corrected\n",
    "\n",
    "    plt.figure()\n",
    "    fig,ax1=plt.subplots()  \n",
    "    plot1=ax1.plot(time, green_detrended, 'b' , label='Green - pre motion correction', alpha=0.5)\n",
    "    plot3=ax1.plot(time, green_corrected, 'g', label='Green - motion corrected', alpha=0.5)\n",
    "    plot4=ax1.plot(time, green_est_motion - 1.05, 'y', label='estimated motion')\n",
    "    \n",
    "    ax1.set_xlabel('Time (seconds)')\n",
    "    ax1.set_ylabel('Green Signal (V)', color='g')\n",
    "    ax1.set_title(alldat[f]['mouseID'])\n",
    "\n",
    "    lines = plot1+plot3+plot4 #+ reward_ticks\n",
    "    labels = [l.get_label() for l in lines]  \n",
    "    legend = ax1.legend(lines, labels, loc='upper right', bbox_to_anchor=(0.95, 0.98))\n",
    "\n",
    "    ax1.set_xlim(1000, 1060)  # 60 sec window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685b4f4",
   "metadata": {},
   "source": [
    "### Z scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34787df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters for Z scoring\n",
    "\n",
    "#Peri-event windows\n",
    "PRE_TIME = 10 #  seconds before event to include\n",
    "POST_TIME = 20 #  seconds after event to include\n",
    "\n",
    "Base_start = -10 # seconds relative to event to start z-score baseline\n",
    "Base_end = -6 # seconds relative to event to end z-score baseline\n",
    "\n",
    "AUC_start = 0 # seconds relative to event to start AUC region\n",
    "AUC_end = 5 # seconds relative to event to end AUC region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to expand TTLs\n",
    "def find_bits(sum_result):\n",
    "    binary_representation = bin(sum_result)[2:][::-1]  # Reverse the binary string\n",
    "    bits = [i for i, bit in enumerate(binary_representation) if bit == '1']\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf99a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting list of bit values and timestamps\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "\n",
    "    bit_values=[]\n",
    "    bit_timestamps=[]\n",
    "\n",
    "    for TTL in range(len(alldat[f]['TTL_values'])):\n",
    "        bits=find_bits(alldat[f]['TTL_values'][TTL].astype(int))\n",
    "        ts=alldat[f]['TTL_timestamps'][TTL]\n",
    "\n",
    "        for bit in range(len(bits)):\n",
    "            bit_values=np.append(bit_values, bits[bit])\n",
    "            bit_timestamps=np.append(bit_timestamps, ts)\n",
    "\n",
    "    alldat[f]['bit_values']=bit_values\n",
    "    alldat[f]['bit_timestamps']=bit_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate peri_time, centered around TTL event\n",
    "\n",
    "fs = alldat[0]['sampling_rate']/N; # must account for downsampling w/ N\n",
    "\n",
    "#time span for peri-event filtering        \n",
    "TRANGE = [-1*PRE_TIME*np.floor(fs),POST_TIME*np.floor(fs)]\n",
    "\n",
    "peri_time = range(int(TRANGE[1]-TRANGE[0]))/fs - PRE_TIME*np.floor(fs)/fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ce991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get snips of trace surrounding each timestamp\n",
    "\n",
    "#this gives the number of datapoints that should be in each snip\n",
    "correct_size = TRANGE[1]-TRANGE[0]\n",
    "\n",
    "for f in range(len(alldat)): \n",
    "\n",
    "    #Get list of bits used in this experiment\n",
    "    Bit_list = sorted(set(alldat[f]['bit_values'].astype(int)))\n",
    "\n",
    "    Bit_snips = {}\n",
    "    Bit_means = {}\n",
    "\n",
    "    #Get snips of trace surrounding each timestamp\n",
    "    for Bit in Bit_list:\n",
    "\n",
    "        Bitname = 'Bit_'+str(Bit)\n",
    "        Beh_bits = np.where(alldat[f]['bit_values'] == Bit)[0]\n",
    "        Beh_t = alldat[f]['bit_timestamps'][Beh_bits]\n",
    "            \n",
    "        trials = len(Beh_t)\n",
    "        dFF_snips = [None] * trials\n",
    "        array_ind = np.zeros(trials)\n",
    "        pre_stim = np.zeros(trials)\n",
    "        post_stim = np.zeros(trials)\n",
    "            \n",
    "        for i in range(trials):\n",
    "                \n",
    "            # Find first time index after bout onset\n",
    "            array_ind[i] = np.argmax(alldat[f]['downsampled_time'] > Beh_t[i])\n",
    "\n",
    "            # Find index corresponding to pre and post stim durations, making sure they're within the bounds of the trace\n",
    "            pre_stim[i] = max(0, (array_ind[i] + TRANGE[0]))\n",
    "            post_stim[i] = min((array_ind[i] + TRANGE[1]), len(alldat[f]['downsampled_time'])) \n",
    "\n",
    "            dFF_snips[i] = alldat[f]['corrected_green'][int(pre_stim[i]):int(post_stim[i])]\n",
    "            \n",
    "\n",
    "        #ignore any NANs or short snips caused by events too close to start or end of trace\n",
    "        dFF_snips = [arr for arr in dFF_snips if isinstance(arr, np.ndarray) and not np.isnan(arr).any()]\n",
    "        dFF_snips = [arr for arr in dFF_snips if len(arr) == correct_size]\n",
    "            \n",
    "        #Convert to a matrix. Store in 'Bit_snips' within each dat                \n",
    "        Bit_snips[Bitname] = np.array(dFF_snips)\n",
    "\n",
    "        #Get mean and stdev for each bit\n",
    "        Bit_means[Bitname] = np.mean(np.array(dFF_snips), axis=0)\n",
    "\n",
    "\n",
    "    alldat[f]['Bit_snips'] = Bit_snips #Call these by 'Bit_0' etc. \n",
    "    alldat[f]['Bit_means'] = Bit_means    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot bits \n",
    "\n",
    "bit_to_plot = 'Bit_1'\n",
    "\n",
    "for f in range(len(alldat)): \n",
    "    \n",
    "    mousenum = alldat[f]['mouseID']\n",
    "\n",
    "    if bit_to_plot in alldat[f]['Bit_means']:\n",
    "        plt.plot(peri_time , alldat[f]['Bit_means'][bit_to_plot], label = str(mousenum+'_'+bit_to_plot))\n",
    "\n",
    "    plt.title('Bit means')\n",
    "    plt.legend(loc='upper left')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21389cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average across groups\n",
    "\n",
    "bit_to_plot = 'Bit_1'\n",
    "\n",
    "\n",
    "for group_id in group_ids:\n",
    "    bit_arrays = []\n",
    "\n",
    "    for f in range(len(alldat)):\n",
    "        if alldat[f]['group']==group_id:\n",
    "            if bit_to_plot in alldat[f]['Bit_means']:\n",
    "                bit_arrays.append(alldat[f]['Bit_means'][bit_to_plot])\n",
    "\n",
    "    # Stack the arrays to compute the average and standard error\n",
    "    bit_stack = np.vstack(bit_arrays)\n",
    "    bit_mean = np.mean(bit_stack, axis=0)\n",
    "    bit_sem = np.std(bit_stack, axis=0) / np.sqrt(bit_stack.shape[0])  # Standard error of the mean\n",
    "\n",
    "    # Plotting the average array with standard error bands\n",
    "    plt.plot(peri_time, bit_mean, label=f'{group_id}')\n",
    "    plt.fill_between(peri_time, bit_mean - bit_sem, bit_mean + bit_sem, alpha=0.3, label='_nolegend_')\n",
    "\n",
    "plt.title('Bit means')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a40ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score everything\n",
    "        \n",
    "# Time windows defined above, relative to timestamp\n",
    "Base_start_calc = (Base_start*np.floor(fs) - TRANGE[0]).astype(int) \n",
    "Base_end_calc = (Base_end*np.floor(fs) - TRANGE[0]).astype(int) \n",
    "\n",
    "AUC_start_calc = (AUC_start*np.floor(fs) - TRANGE[0]).astype(int)\n",
    "AUC_end_calc = (AUC_end*np.floor(fs) - TRANGE[0]).astype(int)\n",
    "\n",
    "for f in range(len(alldat)):  \n",
    "        \n",
    "    z_scores = {}\n",
    "    z_means = {}\n",
    "    z_sterr = {}\n",
    "    z_AUCs = {}\n",
    "    z_AUCmeans = {}\n",
    "    z_means_20 = {}\n",
    "\n",
    "\n",
    "    Bit_list = sorted(set(alldat[f]['bit_values'].astype(int))) \n",
    "\n",
    "    for Bit in Bit_list: \n",
    "\n",
    "        Bitname = 'Bit_'+str(Bit)\n",
    "\n",
    "        trials = len(alldat[f]['Bit_snips'][Bitname])\n",
    "        z_snips = [None] * trials\n",
    "        z_AUC_calc = [None] * trials\n",
    "\n",
    "        for i in range(trials):\n",
    "            zb = np.mean(alldat[f]['Bit_snips'][Bitname][i, Base_start_calc:Base_end_calc]) # baseline period mean \n",
    "            zsd = np.std(alldat[f]['Bit_snips'][Bitname][i, Base_start_calc:Base_end_calc]) # baseline period stdev\n",
    "            z_snips[i]=(alldat[f]['Bit_snips'][Bitname][i,:] - zb)/zsd # Z score for each trial\n",
    "            z_AUC_calc[i] = np.trapz(z_snips[i][AUC_start_calc:AUC_end_calc], peri_time[AUC_start_calc:AUC_end_calc])\n",
    "\n",
    "        #Convert to a matrix. Store in 'Bit_snips' within each dat\n",
    "        z_scores[Bitname] = np.array(z_snips)\n",
    "\n",
    "        #Get mean and stdev for each bit        \n",
    "        z_means[Bitname] = np.mean(np.array(z_snips), axis=0)\n",
    "        z_sterr[Bitname] = np.std(np.array(z_snips), axis=0)/np.sqrt(np.array(z_snips).shape[0])\n",
    "        \n",
    "        #Write AUCs for each bit\n",
    "        z_AUCs[Bitname] = np.array(z_AUC_calc)\n",
    "\n",
    "        z_AUCmeans[Bitname] = np.mean(z_AUCs[Bitname])\n",
    "\n",
    "    alldat[f]['z_scores'] = z_scores \n",
    "    alldat[f]['z_means'] = z_means \n",
    "    alldat[f]['z_sterr'] = z_sterr\n",
    "    alldat[f]['z_AUCs'] = z_AUCs\n",
    "    alldat[f]['z_AUCmeans'] = z_AUCmeans   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b407f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot z scores\n",
    "\n",
    "bit_to_plot = 'Bit_1'\n",
    "\n",
    "for f in range(len(alldat)): \n",
    "    \n",
    "    mousenum = alldat[f]['mouseID']\n",
    "\n",
    "    if bit_to_plot in alldat[f]['z_means']:\n",
    "        plt.plot(peri_time , alldat[f]['z_means'][bit_to_plot], label = str(mousenum+'_'+bit_to_plot))\n",
    "\n",
    "    plt.title('Z means')\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average\n",
    "\n",
    "bit_to_plot = 'Bit_1'\n",
    "\n",
    "for group_id in group_ids:\n",
    "    bit_arrays = []\n",
    "    for f in range(len(alldat)):\n",
    "        if alldat[f]['group']==group_id:\n",
    "            if bit_to_plot in alldat[f]['z_means']:\n",
    "                bit_arrays.append(alldat[f]['z_means'][bit_to_plot])\n",
    "\n",
    "    # Stack the arrays to compute the average and standard error\n",
    "    bit_stack = np.vstack(bit_arrays)\n",
    "    bit_mean = np.mean(bit_stack, axis=0)\n",
    "    bit_sterr = np.std(bit_stack, axis=0) / np.sqrt(bit_stack.shape[0])\n",
    "\n",
    "    # Plotting the average array with error bands\n",
    "    plt.plot(peri_time, bit_mean, label=f'{group_id}')\n",
    "    plt.fill_between(peri_time, bit_mean - bit_sterr, bit_mean + bit_sterr, alpha=0.3)\n",
    "\n",
    "plt.title('Z means')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9130751",
   "metadata": {},
   "source": [
    "### Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export means\n",
    "\n",
    "# Specify which keys to export\n",
    "export_keys = ['z_means', 'Bit_means']  # Add or remove keys as needed\n",
    "\n",
    "# Get the directory name after the final slash\n",
    "directory_name = os.path.basename(data_directory)\n",
    "\n",
    "triallength = len(peri_time)\n",
    "\n",
    "# Sort alldat by group and get sorted column names\n",
    "sorted_indices = sorted(range(len(alldat)), key=lambda i: alldat[i]['group'])\n",
    "colnames = [alldat[i]['mouseID'] for i in sorted_indices] \n",
    "Bit_list = [0,1,2,3,4,5,6] \n",
    "\n",
    "for key in export_keys:\n",
    "    excel_file_path = os.path.join(data_output, f'{directory_name}_{key}.xlsx')\n",
    "    os.makedirs(os.path.dirname(excel_file_path), exist_ok=True)\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "        for Bit in Bit_list: \n",
    "            Bitname = 'Bit_'+str(Bit)  \n",
    "            zframe = np.empty((0, triallength))\n",
    "\n",
    "            # Use sorted_indices to iterate through alldat in group order\n",
    "            for i in sorted_indices:\n",
    "                f = i  # This maintains the original index for accessing alldat[f]\n",
    "                if Bitname in alldat[f].get(key, {}):\n",
    "                    data = alldat[f][key][Bitname]\n",
    "                    # Check if data is empty or entirely NaN\n",
    "                    if data is not None and np.any(~np.isnan(data)):  \n",
    "                        zframe = np.vstack((zframe, data))\n",
    "                    else:\n",
    "                        zframe = np.vstack((zframe, np.full((triallength,), np.nan)))\n",
    "                else:\n",
    "                    zframe = np.vstack((zframe, np.full((triallength,), np.nan)))\n",
    "\n",
    "            df = pd.DataFrame(zframe).T\n",
    "            df.columns = colnames\n",
    "            df.insert(0, 'peri_time', peri_time)\n",
    "            \n",
    "            # Get group labels in the same order as colnames\n",
    "            group_labels = [alldat[i]['group'] for i in sorted_indices]\n",
    "            group_row = [''] + group_labels  # Blank for peri_time column\n",
    "\n",
    "            # Insert the group row as the first row of the DataFrame\n",
    "            df_with_group = pd.concat([\n",
    "                pd.DataFrame([group_row], columns=df.columns),  # group row\n",
    "                df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            sheet_name = Bitname\n",
    "            df_with_group.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 1 workbook per mouse that has bit_snips and zscores for all trials for all bits\n",
    "\n",
    "# Get the directory name after the final slash\n",
    "directory_name = os.path.basename(data_directory)\n",
    "\n",
    "# Create subfolder path\n",
    "subfolder_name = f\"{directory_name}_individual_trials\"\n",
    "subfolder_path = os.path.join(data_output, subfolder_name)\n",
    "os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    \n",
    "    excel_file_path = os.path.join(subfolder_path, f\"{alldat[f]['mouseID']}_trials.xlsx\")\n",
    "\n",
    "    Bit_list = sorted(set(alldat[f]['bit_values'].astype(int)))\n",
    "\n",
    "    with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "        for Bit in Bit_list: \n",
    "\n",
    "            Bitname = 'Bit_'+str(Bit)  \n",
    "            if Bitname in alldat[f]['Bit_snips']:\n",
    "                Bit_snips = pd.DataFrame(alldat[f]['Bit_snips'][Bitname]).T\n",
    "                name1 = str(Bitname +'_Bit_snips')\n",
    "            if Bitname in alldat[f]['z_scores']:\n",
    "                Bit_zscore = pd.DataFrame(alldat[f]['z_scores'][Bitname]).T\n",
    "                name2 = str(Bitname +'_zscores')\n",
    "\n",
    "                Bit_snips.to_excel(writer, sheet_name=name1, index=False)\n",
    "                Bit_zscore.to_excel(writer, sheet_name=name2, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
