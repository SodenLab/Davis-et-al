{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7038af",
   "metadata": {},
   "source": [
    "Used to analyze photometry data when behavior is analyzed with Ethovision or events are flagged using the Synapse Notes feature.\n",
    "Curve fit and motion correction are based on Simpson et al. 2024 (https://doi.org/10.1016/j.neuron.2023.11.016). Cursor AI software was used to assist with writing parts of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt  \n",
    "import scipy.stats as stats\n",
    "\n",
    "import tdt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import os\n",
    "import pylab as plt\n",
    "from scipy.signal import medfilt, butter, filtfilt\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit, minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a1875",
   "metadata": {},
   "source": [
    "### Import Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3b354",
   "metadata": {},
   "source": [
    "Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274da302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indicate experiment type. \n",
    "#If you add a new experiment type, you will need to update EXPERIMENT_CONFIGS and ETHO_COLUMN_CONFIGS\n",
    "experiment_type = 'robobug_manual'\n",
    "\n",
    "# Define experiment configurations (i.e. Ethovision-tracked zones, or anything marked as 1 or 0 in first tab of etho output)\n",
    "# To use the pattern matching in column_config, you need to have the zone name in the column name.\n",
    "EXPERIMENT_CONFIGS = {\n",
    "    'open_field': ['Edge', 'Middle', 'Center'],\n",
    "    'ezm': ['Open', 'Closed'],\n",
    "    'synapse_notes_only': [],\n",
    "    'robobug_manual': []\n",
    "}\n",
    "\n",
    "ETHO_COLUMN_CONFIGS = {  #These reflect the column names in the etho output. They may change if you change Ethovision settings\n",
    "    'open_field': {\n",
    "        'pattern': 'In zone({arena_prefix}{zone} / Center-point)',\n",
    "        'arena_mapping': {\n",
    "            'Arena 1': 'A1',\n",
    "            'Arena 2': 'A2'\n",
    "        }\n",
    "    },\n",
    "    'ezm': {\n",
    "        'pattern': 'In zone ({zone})' \n",
    "    },\n",
    "    'robobug_manual':{\n",
    "        'pattern': '{zone}'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Assign each mouse to a group, quotes around ID number\n",
    "group_id = {\n",
    "    'mouseID':'groupname',\n",
    "    'mouseID':'groupname'\n",
    "    }\n",
    "\n",
    "group_ids = list(set(group_id.values()))\n",
    "\n",
    "#Folder containing 1 day of photometry data from multiple animals\n",
    "data_directory = './Data'\n",
    "\n",
    "#Specify where to save the output. A timestamped subfolder will be created in this folder to hold all output\n",
    "base_output_dir = './Output'\n",
    "\n",
    "#Folder containing exported data from Ethovision. If you don't have Ethovision data, set this to 'no etho data'\n",
    "etho_directory = './Ethovision'\n",
    "\n",
    "#Settingsfor trimming data  \n",
    "t_start = 8 # time threshold below which we will discard (in seconds)\n",
    "t_end_target = 1200 # if video is shorter than this, it will not be trimmed (in seconds)\n",
    "\n",
    "#Downsample rate\n",
    "N = 100 # Average every N samples into 1 value\n",
    "\n",
    "video_sampling_rate=0.05 # Video sampling rate (Hz)\n",
    "\n",
    "#Settings for finding zone and behavior transitions\n",
    "min_duration=1 # seconds they must be in the zone to count as a transition\n",
    "#will also use video_sampling_rate defined above\n",
    "\n",
    "#Set Peri-event window\n",
    "PRE_TIME = 15 #  seconds before event to include\n",
    "POST_TIME = 15 #  seconds after event to include\n",
    "\n",
    "#Set time range for z score baseline and dFF baseline\n",
    "base_start= -15\n",
    "base_end= -11\n",
    "\n",
    "#Set time range for AUC calculations\n",
    "AUC_start= 0\n",
    "AUC_end= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamped subfolder for saving output\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "data_output = os.path.join(base_output_dir, f\"analysis_{timestamp}\")\n",
    "print(f\"Output will be saved to: {data_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all folders in the 'data' directory\n",
    "folderlist = [item for item in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, item))]\n",
    "\n",
    "# Extract metadata from the folder names and read in the data. \n",
    "#Creates 'alldat', a list of structs containing all the data \n",
    "masterdat = []\n",
    "for foldername in folderlist:\n",
    "    dat = {}\n",
    "    \n",
    "    if len(foldername) == 18:\n",
    "        dat['mouse1'] = foldername[0:4]\n",
    "        dat['mouse2'] = '0000'\n",
    "        \n",
    "        dat['date'] = foldername[5:11]\n",
    "        \n",
    "    else:    \n",
    "        dat['mouse1'] = foldername[0:4]\n",
    "        dat['mouse2'] = foldername[5:9]\n",
    "    \n",
    "        dat['date'] = foldername[10:16]\n",
    "\n",
    "    dat['blockpath'] = foldername  # point to tanks (enter file path-this one uses the folders from above)\n",
    "\n",
    "    # Assuming you have TDTbin2mat implemented or accessible\n",
    "    dat['data'] = tdt.read_block(os.path.join(data_directory, dat['blockpath']))\n",
    "    \n",
    "    masterdat.append(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d09a4",
   "metadata": {},
   "source": [
    "### Pulling data and TTLs for each mouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a381e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \"_470A\" etc. values and epocs may need to be changed if recording from a different photometry setup\n",
    "\n",
    "alldat = []\n",
    "\n",
    "for f in range(len(masterdat)):\n",
    "    \n",
    "    if masterdat[f]['mouse1'] != '0000':\n",
    "        dat1 = {}\n",
    "        dat1['mouseID'] = masterdat[f]['mouse1']\n",
    "        dat1['green'] = masterdat[f]['data'].streams._470A.data\n",
    "        dat1['isos'] = masterdat[f]['data'].streams._405A.data\n",
    "        dat1['sampling_rate'] = masterdat[f]['data'].streams._470A.fs\n",
    "        dat1['photom_time'] = (np.arange(1,len(dat1['green'])+1))/dat1['sampling_rate']\n",
    "        dat1['video_time'] = masterdat[f]['data'].epocs.Cam1.onset\n",
    "        if experiment_type == 'synapse_notes_only':\n",
    "            dat1['ttl_notes'] = masterdat[f]['data'].epocs.Note.data.astype(int)\n",
    "            dat1['ttl_notes_time'] = masterdat[f]['data'].epocs.Note.onset\n",
    "        alldat.append(dat1)\n",
    "    \n",
    "    if masterdat[f]['mouse2'] != '0000':\n",
    "        dat2 = {}\n",
    "        dat2['mouseID'] = masterdat[f]['mouse2']\n",
    "        dat2['green'] = masterdat[f]['data'].streams._470B.data\n",
    "        dat2['isos'] = masterdat[f]['data'].streams._405B.data\n",
    "        dat2['sampling_rate'] = masterdat[f]['data'].streams._470B.fs\n",
    "        dat2['photom_time'] = (np.arange(1,len(dat2['green'])+1))/dat2['sampling_rate']\n",
    "        dat2['video_time']=masterdat[f]['data'].epocs.Cam1.onset\n",
    "        if experiment_type == 'synapse_notes_only':\n",
    "            dat2['ttl_notes'] = masterdat[f]['data'].epocs.Note.data.astype(int)\n",
    "            dat2['ttl_notes_time'] = masterdat[f]['data'].epocs.Note.onset\n",
    "        alldat.append(dat2)\n",
    "\n",
    "#assigning group name\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    alldat[f]['group'] = group_id[alldat[f]['mouseID']]\n",
    "\n",
    "# cut so length matches\n",
    "    \n",
    "for f in range(len(alldat)):\n",
    "    \n",
    "    a = len(alldat[f]['green'])\n",
    "    b = len(alldat[f]['isos'])\n",
    "    if b < a:\n",
    "        alldat[f]['green'] = alldat[f]['green'][0:b]\n",
    "    if a < b:\n",
    "        alldat[f]['isos'] = alldat[f]['isos'][0:a]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce69b6",
   "metadata": {},
   "source": [
    "### Importing Ethovision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011743d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Ethovision files if using Synapse Notes Only\n",
    "\n",
    "print(f\"Current experiment_type is: '{experiment_type}'\")\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No Etho files for synapse_notes_only')\n",
    "\n",
    "#The rest of the block will run for other experiment types\n",
    "else:\n",
    "    folder_path = etho_directory\n",
    "    arenas = {}\n",
    "    etho_output = {}\n",
    "    etho_output_manual = {}  # separate dictionary for manual scoring sheets\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".xlsx\"):  \n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            xls = pd.ExcelFile(file_path)  # Read the Excel file\n",
    "                \n",
    "            for sheet_name in xls.sheet_names:\n",
    "                df = pd.read_excel(xls, sheet_name=sheet_name, header=None)  # Read each sheet without headers\n",
    "                    \n",
    "                # Find the row where \"Eartag\" is in column A (column index 0)\n",
    "                eartag_row = df[df[0] == \"Eartag\"]\n",
    "                if not eartag_row.empty:\n",
    "                    eartag = str(eartag_row.iloc[0, 1])  # Get the value from column B (column index 1)\n",
    "                else:\n",
    "                    # If \"Eartag\" not found, search for \"Animal ID\" instead\n",
    "                    animal_id_row = df[df[0] == \"Animal ID\"]\n",
    "                    if not animal_id_row.empty:\n",
    "                        eartag = str(animal_id_row.iloc[0, 1])  # Get the value from column B (column index 1)\n",
    "                    else:\n",
    "                        continue  # Skip if neither \"Eartag\" nor \"Animal ID\" found\n",
    "                        \n",
    "                # Find the row where \"Arena\" is in column A\n",
    "                arena_row = df[df[0] == \"Arena name\"]\n",
    "                if not arena_row.empty:\n",
    "                    arena = str(arena_row.iloc[0, 1])  # Get the value from column B (column index 1)\n",
    "                else:\n",
    "                    continue  # Skip if no \"Arena\" found           \n",
    "                \n",
    "                # Find the row where \"Trial time\" is in column A\n",
    "                trial_time_row = df[df[0] == \"Trial time\"]\n",
    "                if not trial_time_row.empty:\n",
    "                    trial_start_index = trial_time_row.index[0]  # Get the row index\n",
    "                        \n",
    "                    # Ensure there is a row after \"Trial time\" before attempting to drop\n",
    "                    if trial_start_index + 1 < len(df):\n",
    "                        df = df.drop(index=trial_start_index + 1)  # Drop the row after \"Trial time\"\n",
    "                        \n",
    "                    # Extract from \"Trial time\" row onward\n",
    "                    trial_df = df.iloc[trial_start_index:].reset_index(drop=True)\n",
    "                        \n",
    "                    # Set first row as column headers\n",
    "                    trial_df.columns = trial_df.iloc[0]  \n",
    "                    trial_df = trial_df[1:].reset_index(drop=True)  # Remove header row\n",
    "                else:\n",
    "                    continue  # Skip if no \"Trial time\" found\n",
    "                    \n",
    "                # Store DataFrame in appropriate dictionary based on sheet name\n",
    "                if \"Manual\" in sheet_name:\n",
    "                    etho_output_manual[eartag] = trial_df\n",
    "                else:\n",
    "                    etho_output[eartag] = trial_df\n",
    "                \n",
    "                arenas[eartag] = arena\n",
    "\n",
    "    # Now `etho_output` holds automated scoring DataFrames and `etho_output_manual` holds manual scoring DataFrames\n",
    "    print(f\"Processed {len(etho_output)} automated scoring sheets and {len(etho_output_manual)} manual scoring sheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fedae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign etho data to alldat\n",
    "\n",
    "print(f\"Current experiment_type is: '{experiment_type}'\")\n",
    "print()\n",
    "\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No Etho files for synapse_notes_only')\n",
    "\n",
    "else:\n",
    "    for entry in alldat:\n",
    "        mouse_id = entry['mouseID']  # Extract mouseID from alldat\n",
    "        print(f\"mouse_id:{mouse_id}\")\n",
    "        if mouse_id in etho_output:  # Check if it matches an Eartag in data_dict\n",
    "            entry['etho_raw_data'] = etho_output[mouse_id]  # Assign the matching DataFrame\n",
    "            print(\" etho_output saved\")\n",
    "        else:\n",
    "            entry['etho_raw_data'] = None  # Optional: Assign None if no match is found\n",
    "            print(\" etho_output not saved\")\n",
    "\n",
    "        if mouse_id in etho_output_manual:  # Check if it matches an Eartag in data_dict\n",
    "            entry['etho_manual_data'] = etho_output_manual[mouse_id]  # Assign the matching DataFrame\n",
    "            print(\" etho_output_manual saved\")\n",
    "        else:\n",
    "            entry['etho_manual_data'] = None  # Optional: Assign None if no match is found\n",
    "            print(\" etho_output_manual not saved\")\n",
    "\n",
    "        if mouse_id in arenas:  # Check if it matches an Eartag in data_dict\n",
    "            entry['arena'] = arenas[mouse_id]  # Assign the matching DataFrame\n",
    "            print(f\" arena is {entry['arena']}\")\n",
    "        else:\n",
    "            entry['arena'] = None  # Optional: Assign None if no match is found\n",
    "            print(\" no arena found\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3aff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out velocity and time data\n",
    "\n",
    "print(f\"Current experiment_type is: '{experiment_type}'\")\n",
    "print()\n",
    "\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No Etho files for synapse_notes_only')\n",
    "\n",
    "else:\n",
    "    for entry in alldat:\n",
    "        print(f\"mouse_id:{entry['mouseID']}\")\n",
    "        if 'etho_raw_data' in entry and isinstance(entry['etho_raw_data'], pd.DataFrame):\n",
    "            if 'Velocity' in entry['etho_raw_data'].columns:\n",
    "                entry['velocity'] = entry['etho_raw_data']['Velocity'].values \n",
    "                print(\" velocity saved\")\n",
    "            else:\n",
    "                entry['velocity'] = []  # Assign an empty list if the column is missing\n",
    "                print(\" velocity not saved\")\n",
    "        else:\n",
    "            entry['velocity'] = []  # Assign an empty list if 'etho_raw_data' is missing or not a DataFrame  \n",
    "            print(\" velocity not saved\")\n",
    "\n",
    "        if 'etho_raw_data' in entry and isinstance(entry['etho_raw_data'], pd.DataFrame):\n",
    "            if 'Trial time' in entry['etho_raw_data'].columns:\n",
    "                entry['Etho_time'] = entry['etho_raw_data']['Trial time'].values \n",
    "                print(\" Etho_time saved\")\n",
    "            else:\n",
    "                entry['Etho_time'] = []  # Assign an empty list if the column is missing\n",
    "                print(\" Etho_time not saved\")\n",
    "        else:\n",
    "            entry['Etho_time'] = []  # Assign an empty list if 'etho_raw_data' is missing or not a DataFrame \n",
    "            print(\" Etho_time not saved\")\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out zone data\n",
    "\n",
    "print(f\"Current experiment_type is: '{experiment_type}'\")\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No Etho files for synapse_notes_only')\n",
    "\n",
    "else:\n",
    "    # Get zones and column config for current experiment type\n",
    "    zones = EXPERIMENT_CONFIGS[experiment_type]\n",
    "    column_config = ETHO_COLUMN_CONFIGS[experiment_type]\n",
    "    \n",
    "    # Universal function to extract zone data\n",
    "    def extract_zone_data(entry, zones, column_config):\n",
    "        saved_info = []\n",
    "        for zone in zones:\n",
    "            column_name = None\n",
    "            \n",
    "            # Use pattern with arena prefix (like open_field)\n",
    "            if 'arena_mapping' in column_config:\n",
    "                arena = entry.get('arena', '')\n",
    "                arena_prefix = column_config['arena_mapping'].get(arena, '')\n",
    "                column_name = column_config['pattern'].format(zone=zone, arena_prefix=arena_prefix)\n",
    "            \n",
    "            # Use simple pattern (like NISF and EZM)\n",
    "            else:\n",
    "                column_name = column_config['pattern'].format(zone=zone)\n",
    "            \n",
    "            # Extract the data if we found a valid column name\n",
    "            if column_name and column_name in entry['etho_raw_data'].columns:\n",
    "                entry[zone] = entry['etho_raw_data'][column_name].values\n",
    "                saved_info.append(f\"{zone} from column [{column_name}]\")\n",
    "        \n",
    "        return saved_info\n",
    "\n",
    "    # Main processing loop\n",
    "    for entry in alldat:\n",
    "        if 'etho_raw_data' in entry and isinstance(entry['etho_raw_data'], pd.DataFrame):\n",
    "            saved_info = extract_zone_data(entry, zones, column_config)\n",
    "            mouse_id = entry.get('mouseID', 'Unknown')\n",
    "            arena = entry.get('arena', 'No arena')\n",
    "            print(f\"Mouse {mouse_id} ({arena}):\")\n",
    "            if saved_info:\n",
    "                for info in saved_info:\n",
    "                    print(f\"    {info}\")\n",
    "            else:\n",
    "                print(\"    No zones saved\")\n",
    "        else:\n",
    "            mouse_id = entry.get('mouseID', 'Unknown')\n",
    "            print(f\"Mouse {mouse_id}: No Etho data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check ethovision additions; length should match the number of entries on ethovision raw data output\n",
    "\n",
    "print(f\"Current experiment_type is: '{experiment_type}'\")\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No Etho files for synapse_notes_only')\n",
    "\n",
    "else:\n",
    "    # Get zones for current experiment type\n",
    "    zones = EXPERIMENT_CONFIGS[experiment_type]\n",
    "    \n",
    "    for f in range(len(alldat)):\n",
    "        print(f\"eartag {alldat[f]['mouseID']}\")\n",
    "        \n",
    "        # Check velocity if available\n",
    "        if 'velocity' in alldat[f] and len(alldat[f]['velocity']) > 0:\n",
    "            print(f\"    velocity array length {len(alldat[f]['velocity'])}\")\n",
    "        \n",
    "        # Check etho time if available\n",
    "        if 'Etho_time' in alldat[f] and len(alldat[f]['Etho_time']) > 0:\n",
    "            print(f\"    etho time array length {len(alldat[f]['Etho_time'])}\")\n",
    "        \n",
    "        # Check zone data dynamically\n",
    "        for zone in zones:\n",
    "            if zone in alldat[f] and len(alldat[f][zone]) > 0:\n",
    "                print(f\"    {zone.lower()} array length {len(alldat[f][zone])}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9db03d1",
   "metadata": {},
   "source": [
    "### Remove artifact at start and downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7456a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove artifact at start, also trim ethovision data since it's time aligned to video_time\n",
    "\n",
    "zones = EXPERIMENT_CONFIGS[experiment_type]  # get the zones for current experiment type\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    t_end = min(max(alldat[f]['video_time']), t_end_target)  \n",
    "    \n",
    "    idx_start_photom = np.argmax(alldat[f]['photom_time'] > t_start)\n",
    "    idx_start_video = np.argmax(alldat[f]['video_time'] > t_start)\n",
    "    idx_end_video = np.argmax(alldat[f]['video_time'] > t_end) if t_end < max(alldat[f]['video_time']) else len(alldat[f]['video_time'])\n",
    "    idx_end_photom = np.argmax(alldat[f]['photom_time'] > t_end) if t_end < max(alldat[f]['photom_time']) else len(alldat[f]['photom_time'])\n",
    "   \n",
    "    alldat[f]['trimmed_photom_time'] = alldat[f]['photom_time'][idx_start_photom:idx_end_photom]\n",
    "    alldat[f]['trimmed_green'] = alldat[f]['green'][idx_start_photom:idx_end_photom]\n",
    "    alldat[f]['trimmed_isos'] = alldat[f]['isos'][idx_start_photom:idx_end_photom]\n",
    "    alldat[f]['trimmed_video_time'] = alldat[f]['video_time'][idx_start_video:idx_end_video]\n",
    "\n",
    "    if experiment_type != 'synapse_notes_only':\n",
    "        alldat[f]['trimmed_velocity'] = alldat[f]['velocity'][idx_start_video:idx_end_video]\n",
    "        alldat[f]['trimmed_Etho_time'] = alldat[f]['Etho_time'][idx_start_video:idx_end_video]\n",
    "    \n",
    "    # Trim zone data based on experiment type\n",
    "    for zone in zones:\n",
    "        alldat[f][f'trimmed_{zone}'] = alldat[f][zone][idx_start_video:idx_end_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d25d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check trims and make sure they are the same length\n",
    "# camera length should match ethovision velocity and zone lengths\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    print(f\"eartag {alldat[f]['mouseID']}\")\n",
    "    print(f\"photom_time length {len(alldat[f]['trimmed_photom_time'])}  photom_time start {alldat[f]['trimmed_photom_time'][0]}  photom_time end {alldat[f]['trimmed_photom_time'][len(alldat[f]['trimmed_photom_time'])-1]}\")\n",
    "    print(f\"video_time length {len(alldat[f]['trimmed_video_time'])}  video_time start {alldat[f]['trimmed_video_time'][0]}  video_time end {alldat[f]['trimmed_video_time'][len(alldat[f]['trimmed_video_time'])-1]}\")\n",
    "    if experiment_type != 'synapse_notes_only':\n",
    "        print(f\"velocity length {len(alldat[f]['trimmed_velocity'])}\")\n",
    "        print(f\"Etho time length {len(alldat[f]['trimmed_Etho_time'])}\")\n",
    "    \n",
    "    zones = EXPERIMENT_CONFIGS[experiment_type]\n",
    "    for zone in zones:\n",
    "        print(f\"{zone} length {len(alldat[f][f'trimmed_{zone}'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20267c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample and average \n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    F_green=[]\n",
    "    for i in range(0, len(alldat[f]['trimmed_green']), N):\n",
    "        small_list = np.mean(alldat[f]['trimmed_green'][i:i+N-1])\n",
    "        F_green.append(small_list)\n",
    "    alldat[f]['downsampled_green'] = np.array(F_green)\n",
    "    \n",
    "    F_isos=[]\n",
    "    for i in range(0, len(alldat[f]['trimmed_isos']), N):\n",
    "        small_lst = np.mean(alldat[f]['trimmed_isos'][i:i+N-1])\n",
    "        F_isos.append(small_lst)\n",
    "    alldat[f]['downsampled_isos'] = np.array(F_isos)\n",
    "    \n",
    "    alldat[f]['downsampled_photom_time'] = alldat[f]['trimmed_photom_time'][::N]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0c243",
   "metadata": {},
   "source": [
    "### Fit double exponential to each curve and use to correct for bleaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb990de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exponential(t, const, amp_fast, amp_slow, tau_slow, tau_multiplier):\n",
    "    \"\"\"Compute a double exponential function with constant offset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array-like\n",
    "        Time vector in seconds\n",
    "    const : float\n",
    "        Amplitude of the constant offset\n",
    "    amp_fast : float\n",
    "        Amplitude of the fast component\n",
    "    amp_slow : float\n",
    "        Amplitude of the slow component\n",
    "    tau_slow : float\n",
    "        Time constant of slow component in seconds\n",
    "    tau_multiplier : float\n",
    "        Time constant of fast component relative to slow (tau_fast = tau_slow * tau_multiplier)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        The computed double exponential values at each time point\n",
    "    \"\"\"\n",
    "    tau_fast = tau_slow * tau_multiplier\n",
    "    return const + amp_slow * np.exp(-t/tau_slow) + amp_fast * np.exp(-t/tau_fast)\n",
    "\n",
    "def get_bounds(trace, timecourse, tau_slow_min=0.0001, tau_slow_max=30):\n",
    "    \"\"\"Calculate parameter bounds for double exponential fitting.\n",
    "    \n",
    "    This function determines appropriate bounds for the parameters used in double exponential\n",
    "    fitting based on the characteristics of the input signal and time vector.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trace : array-like\n",
    "        The signal trace to be fit\n",
    "    timecourse : array-like\n",
    "        Time vector corresponding to the trace data points\n",
    "    tau_slow_min : float, optional\n",
    "        Minimum value for tau_slow as a fraction of total time duration (default: 0.0001)\n",
    "    tau_slow_max : float, optional\n",
    "        Maximum value for tau_slow as a fraction of total time duration (default: 30)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of lists\n",
    "        Two lists containing the lower and upper bounds respectively for:\n",
    "        [amp_min, amp_min, amp_min, time_constant_min, offset_min],\n",
    "        [amp_max, amp_max, amp_max, time_constant_max, offset_max]\n",
    "        These bounds are used for fitting the double exponential function parameters.\n",
    "    \"\"\"\n",
    "    signal = trace\n",
    "    \n",
    "    # Amplitude bounds\n",
    "    amp_min = 0  # Assuming amplitude cannot be negative\n",
    "    amp_max = 2 * np.max(signal)  # Allowing for some flexibility\n",
    "\n",
    "    # Time constant bounds based on the duration of the experiment\n",
    "    time_constant_min = tau_slow_min * (timecourse[-1] - timecourse[0])  # Minimum time constant\n",
    "    time_constant_max = tau_slow_max * (timecourse[-1] - timecourse[0])  # Maximum time constant\n",
    "\n",
    "    # Offset bounds\n",
    "    offset_min = np.min(signal) if np.min(signal) < 0 else 0  # Adjust based on signal characteristics\n",
    "    offset_max = np.max(signal)\n",
    "\n",
    "    return (\n",
    "        [amp_min, amp_min, amp_min, time_constant_min, offset_min],\n",
    "        [amp_max, amp_max, amp_max, time_constant_max, offset_max]\n",
    "    )\n",
    "\n",
    "def process_photometry_signals(alldat):\n",
    "    \"\"\"Process photometry signals by fitting and removing exponential bleaching curves.\n",
    "    \n",
    "    For each entry in alldat, fits double exponential curves to both signals and\n",
    "    subtracts them to correct for bleaching. Creates side-by-side plots showing\n",
    "    raw signals with fits and detrended signals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alldat : list of dict\n",
    "        List of data dictionaries containing photometry signals\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Modifies alldat in place, adding:\n",
    "        - 'expfitgreen', 'expfitisos': fitted curves\n",
    "        - 'detrended_green', 'detrended_isos': bleaching-corrected signals\n",
    "    \"\"\"\n",
    "    for f in range(len(alldat)):\n",
    "        # Get signals\n",
    "        green = alldat[f]['downsampled_green']\n",
    "        isos = alldat[f]['downsampled_isos']\n",
    "        time = alldat[f]['downsampled_photom_time']\n",
    "        \n",
    "        # Fit green signal\n",
    "        max_sig = np.max(green)\n",
    "        inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "        bounds = get_bounds(green, time)\n",
    "        green_parms, _ = curve_fit(double_exponential, time, green, \n",
    "                                 p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "        green_expfit = double_exponential(time, *green_parms)\n",
    "        alldat[f]['expfitgreen'] = green_expfit\n",
    "        \n",
    "        # Fit isosbestic signal\n",
    "        max_sig = np.max(isos)\n",
    "        inital_params = [max_sig/2, max_sig/4, max_sig/4, 3600, 0.1]\n",
    "        bounds = get_bounds(isos, time)\n",
    "        isos_parms, _ = curve_fit(double_exponential, time, isos, \n",
    "                                 p0=inital_params, bounds=bounds, maxfev=1000)\n",
    "        isos_expfit = double_exponential(time, *isos_parms)\n",
    "        alldat[f]['expfitisos'] = isos_expfit\n",
    "        \n",
    "        # Detrend signals\n",
    "        green_detrended = green - green_expfit\n",
    "        isos_detrended = isos - isos_expfit\n",
    "        alldat[f]['detrended_green'] = green_detrended\n",
    "        alldat[f]['detrended_isos'] = isos_detrended\n",
    "\n",
    "        # Create figure with two subplots side by side\n",
    "        fig = plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        # Plot 1: Raw signals with fits\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        plot1 = ax1.plot(time, green, 'g', label='green')\n",
    "        plot3 = ax1.plot(time, green_expfit, 'k', linewidth=1.5, label='Exponential fit')\n",
    "        ax1.set_xlabel('Time (seconds)')\n",
    "        ax1.set_ylabel('Green Signal (V)', color='g')\n",
    "        ax1.tick_params(axis='y', labelcolor='g')\n",
    "        \n",
    "        ax1_twin = ax1.twinx()\n",
    "        plot2 = ax1_twin.plot(time, isos, 'b', label='isos')\n",
    "        plot4 = ax1_twin.plot(time, isos_expfit, 'k', linewidth=1.5)\n",
    "        ax1_twin.set_ylabel('Isos Signal (V)', color='b')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='b')\n",
    "        \n",
    "        # Combine legends\n",
    "        lines = plot1 + plot2 + plot3\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax1.legend(lines, labels, loc='upper right')\n",
    "        ax1.set_title(f\"{alldat[f]['mouseID']} - Raw Signals\")\n",
    "        \n",
    "        # Plot 2: Detrended signals\n",
    "        ax2 = plt.subplot(1, 2, 2)\n",
    "        plot5 = ax2.plot(time, green_detrended, 'g', label='green')\n",
    "        ax2.set_xlabel('Time (seconds)')\n",
    "        ax2.set_ylabel('Green Signal (V)', color='g')\n",
    "        ax2.tick_params(axis='y', labelcolor='g')\n",
    "        \n",
    "        ax2_twin = ax2.twinx()\n",
    "        plot6 = ax2_twin.plot(time, isos_detrended, 'b', label='isos')\n",
    "        ax2_twin.set_ylabel('Isos Signal (V)', color='b')\n",
    "        ax2_twin.tick_params(axis='y', labelcolor='b')\n",
    "        \n",
    "        # Combine legends\n",
    "        lines = plot5 + plot6\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax2.legend(lines, labels, loc='upper right')\n",
    "        ax2.set_title(f\"{alldat[f]['mouseID']} - Detrended Signals\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "\n",
    "def plot_motion_correction(time, isos_detrended, green_detrended, green_corrected, green_est_motion, slope, r_value, mouseID):\n",
    "    \"\"\"Plot motion correction analysis with correlation and correction plots side by side.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array-like\n",
    "        Time points for x-axis\n",
    "    isos_detrended : array-like\n",
    "        Detrended isosbestic signal\n",
    "    green_detrended : array-like\n",
    "        Detrended green signal before motion correction\n",
    "    green_corrected : array-like\n",
    "        Motion-corrected green signal\n",
    "    green_est_motion : array-like\n",
    "        Estimated motion artifact\n",
    "    slope : float\n",
    "        Regression slope between isos and green\n",
    "    r_value : float\n",
    "        Correlation coefficient\n",
    "    mouseID : str\n",
    "        Mouse identifier for plot title\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Correlation scatter plot\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.scatter(isos_detrended[::5], green_detrended[::5], alpha=0.1, marker='.')\n",
    "    x = np.array([np.min(isos_detrended), np.max(isos_detrended)])\n",
    "    intercept = np.mean(green_detrended) - slope * np.mean(isos_detrended)\n",
    "    ax1.plot(x, intercept + slope*x, 'b', linewidth=2)  # Changed from 'r' to 'b'\n",
    "    ax1.set_xlabel('Isos')\n",
    "    ax1.set_ylabel('Green')\n",
    "    ax1.set_title(f\"{mouseID} - Motion Correlation\\nSlope: {slope:.3f}, RÂ²: {r_value**2:.3f}\")\n",
    "    \n",
    "    # Plot 2: Signal correction\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    plot1 = ax2.plot(time, green_detrended, 'b', label='Pre motion correction', alpha=0.5)\n",
    "    plot2 = ax2.plot(time, green_corrected, 'g', label='Motion corrected', alpha=0.5)\n",
    "    plot3 = ax2.plot(time, green_est_motion - 1.05, 'y', label='Estimated motion')\n",
    "    \n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.set_ylabel('Green Signal (V)')\n",
    "    ax2.set_title(f\"{mouseID} - Motion Correction\")\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_xlim(60, 120)  # 60 sec window\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def process_motion_correction(alldat):\n",
    "    \"\"\"Process motion correction for all recordings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alldat : list of dict\n",
    "        List of data dictionaries containing photometry signals\n",
    "    \"\"\"\n",
    "    for f in range(len(alldat)):\n",
    "        # Get detrended signals\n",
    "        green_detrended = alldat[f]['detrended_green']\n",
    "        isos_detrended = alldat[f]['detrended_isos']\n",
    "        time = alldat[f]['downsampled_photom_time']\n",
    "\n",
    "        # Calculate correlation between signals\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x=isos_detrended, y=green_detrended)\n",
    "        print(f\"\\nMouse {alldat[f]['mouseID']}:\")\n",
    "        print('Slope    : {:.3f}'.format(slope))\n",
    "        print('R-squared: {:.3f}'.format(r_value**2))\n",
    "\n",
    "        # Calculate and remove motion effect\n",
    "        green_est_motion = intercept + slope * isos_detrended\n",
    "        green_corrected = green_detrended - green_est_motion\n",
    "        alldat[f]['corrected_green'] = green_corrected\n",
    "\n",
    "        # Plot correlation and correction\n",
    "        plot_motion_correction(time, isos_detrended, green_detrended, \n",
    "                             green_corrected, green_est_motion, \n",
    "                             slope, r_value, alldat[f]['mouseID']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328fbef",
   "metadata": {},
   "source": [
    "Curve fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a53e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and plot photometry signals\n",
    "process_photometry_signals(alldat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c11d50d",
   "metadata": {},
   "source": [
    "Motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process motion correction\n",
    "process_motion_correction(alldat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685b4f4",
   "metadata": {},
   "source": [
    "### Map event timestamps to photometry time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a264b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to identify entries into zones. Can also be used for any other data output as 1's and 0's, such as freezing\n",
    "\n",
    "def find_zeros_to_ones(arr, min_duration, sampling_rate): \n",
    "    min_samples = int(min_duration / sampling_rate)\n",
    "    arr = np.array(arr)\n",
    "    valid_mask = (arr == 0) | (arr == 1)\n",
    "    filtered_arr = arr[valid_mask]\n",
    "    \n",
    "    # Indices in the filtered (cleaned) array where transition occurs\n",
    "    transition_indices = np.where(np.diff(filtered_arr) == 1)[0] + 1\n",
    "    \n",
    "    # Check that the value after transition stays 1 for at least min_duration\n",
    "    valid_transitions = []\n",
    "    for idx in transition_indices:\n",
    "        # Count how many 1s follow this index\n",
    "        count = 1\n",
    "        while (idx + count < len(filtered_arr)) and (filtered_arr[idx + count] == 1):\n",
    "            count += 1\n",
    "        if count >= min_samples:\n",
    "            valid_transitions.append(idx)\n",
    "    \n",
    "    original_indices = np.flatnonzero(valid_mask)\n",
    "    return original_indices[valid_transitions]\n",
    "\n",
    "def find_ones_to_zeros(arr, min_duration, sampling_rate): \n",
    "    min_samples = int(min_duration / sampling_rate)\n",
    "    arr = np.array(arr)\n",
    "    valid_mask = (arr == 0) | (arr == 1)\n",
    "    filtered_arr = arr[valid_mask]\n",
    "    \n",
    "    transition_indices = np.where(np.diff(filtered_arr) == -1)[0] + 1\n",
    "    \n",
    "    valid_transitions = []\n",
    "    for idx in transition_indices:\n",
    "        count = 1\n",
    "        while (idx + count < len(filtered_arr)) and (filtered_arr[idx + count] == 0):\n",
    "            count += 1\n",
    "        if count >= min_duration:\n",
    "            valid_transitions.append(idx)\n",
    "    \n",
    "    original_indices = np.flatnonzero(valid_mask)\n",
    "    return original_indices[valid_transitions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b452fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of zone entries and exits. \n",
    "\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    print('No zone data for synapse_notes_only')\n",
    "\n",
    "else:       \n",
    "    for f in range(len(alldat)):\n",
    "        zones = EXPERIMENT_CONFIGS[experiment_type]\n",
    "        \n",
    "        print(f\"Mouse {alldat[f]['mouseID']}:\")\n",
    "        \n",
    "        # Process each zone\n",
    "        for zone in zones:\n",
    "            zone_status = alldat[f][f'trimmed_{zone}']\n",
    "            zone_entries = find_zeros_to_ones(zone_status, min_duration=min_duration, sampling_rate=video_sampling_rate) \n",
    "            zone_exits = find_ones_to_zeros(zone_status, min_duration=min_duration, sampling_rate=video_sampling_rate)\n",
    "            \n",
    "            # Save these onsets back into data structure \n",
    "            entry_key = f'{zone.lower()}_entries'\n",
    "            exit_key = f'{zone.lower()}_exits'\n",
    "            alldat[f][entry_key] = zone_entries\n",
    "            alldat[f][exit_key] = zone_exits\n",
    "            \n",
    "            # Print summary for each zone\n",
    "            print(f\"  {zone}: {len(zone_entries)} entries, {len(zone_exits)} exits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling out synapse ttl notes timestamps. If you have multiple types of notes, they will be separated into 'bit1', 'bit2', etc.\n",
    "\n",
    "if experiment_type != 'synapse_notes_only':\n",
    "    print('No TTL data')\n",
    "\n",
    "else:\n",
    "    for f in range(len(alldat)): \n",
    "        bitvalues = [int(str(TTL)[0]) for TTL in alldat[f]['ttl_notes']]\n",
    "        timestamps = alldat[f]['ttl_notes_time']\n",
    "        \n",
    "        # Get unique bit values\n",
    "        unique_bits = np.unique(bitvalues)\n",
    "        \n",
    "        print(f\"Dataset {f} (Mouse {alldat[f]['mouseID']}):\")\n",
    "        print(f\"  Found bits: {unique_bits}\")\n",
    "        \n",
    "        # Create a dictionary or separate arrays for each unique bit value\n",
    "        for bit in unique_bits:\n",
    "            # Create array name like 'bit1_timestamps', 'bit2_timestamps', etc.\n",
    "            array_name = f'bit{bit}'\n",
    "            # Get timestamps where bitvalues equals this bit\n",
    "            bit_timestamps = timestamps[np.array(bitvalues) == bit]\n",
    "            alldat[f][array_name] = bit_timestamps\n",
    "            \n",
    "            # Print summary for each bit\n",
    "            print(f\"  {array_name}: {len(bit_timestamps)} timestamps\")\n",
    "            \n",
    "        # Keep the original arrays \n",
    "        alldat[f]['bit_values'] = np.array(bitvalues)\n",
    "        alldat[f]['bit_timestamps'] = timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying manual scoring events\n",
    "\n",
    "# First, collect all unique behaviors from all mice\n",
    "all_unique_behaviors_base = set()\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    #Check if entry has manual data\n",
    "    if 'etho_manual_data' in alldat[f] and isinstance(alldat[f]['etho_manual_data'], pd.DataFrame):\n",
    "        unique_behaviors_base = alldat[f]['etho_manual_data']['Behavior'].unique()\n",
    "        all_unique_behaviors_base.update(unique_behaviors_base)\n",
    "\n",
    "# Convert to sorted list for consistency\n",
    "all_unique_behaviors_base = sorted(list(all_unique_behaviors_base))\n",
    "\n",
    "# Expand unique_behaviors to include start and stop variants for ALL behaviors\n",
    "all_unique_behaviors = []\n",
    "for behavior in all_unique_behaviors_base:\n",
    "    all_unique_behaviors.append(behavior)\n",
    "    all_unique_behaviors.append(f\"{behavior}_start\")\n",
    "    all_unique_behaviors.append(f\"{behavior}_stop\")\n",
    "\n",
    "# Now process each mouse's data\n",
    "for f in range(len(alldat)):\n",
    "    #Check if entry has manual data\n",
    "    if 'etho_manual_data' in alldat[f] and isinstance(alldat[f]['etho_manual_data'], pd.DataFrame):\n",
    "        etho_time = alldat[f]['trimmed_Etho_time']\n",
    "        #Get unique behaviors for this mouse\n",
    "        unique_behaviors_base = alldat[f]['etho_manual_data']['Behavior'].unique()\n",
    "        \n",
    "        print(f\"Mouse {alldat[f]['mouseID']}:\")\n",
    "        \n",
    "        #Create arrays for each behavior and store in alldat\n",
    "        for behavior_base in unique_behaviors_base:\n",
    "            # Get timestamps for this behavior\n",
    "            behavior_data = alldat[f]['etho_manual_data'][\n",
    "                alldat[f]['etho_manual_data']['Behavior'] == behavior_base\n",
    "            ]\n",
    "            \n",
    "            # Separate start and stop times\n",
    "            start_times = behavior_data[\n",
    "                behavior_data['Event'] == 'state start'\n",
    "            ]['Trial time'].values\n",
    "            \n",
    "            stop_times = behavior_data[\n",
    "                behavior_data['Event'] == 'state stop'\n",
    "            ]['Trial time'].values\n",
    "            \n",
    "            # Store start times\n",
    "            alldat[f][f\"{behavior_base}_start_times\"] = start_times\n",
    "            \n",
    "            # Store stop times\n",
    "            alldat[f][f\"{behavior_base}_stop_times\"] = stop_times\n",
    "            \n",
    "            # Process start events\n",
    "            if len(start_times) > 0:\n",
    "                start_indices = np.searchsorted(etho_time, start_times)\n",
    "                start_indices = np.clip(start_indices, 0, len(etho_time) - 1)\n",
    "                mask = (start_indices > 0) & (np.abs(etho_time[start_indices-1] - start_times) < \n",
    "                                              np.abs(etho_time[start_indices] - start_times))\n",
    "                start_indices[mask] -= 1\n",
    "                alldat[f][f\"{behavior_base}_start\"] = start_indices\n",
    "            else:\n",
    "                alldat[f][f\"{behavior_base}_start\"] = np.array([])\n",
    "            \n",
    "            # Process stop events\n",
    "            if len(stop_times) > 0:\n",
    "                stop_indices = np.searchsorted(etho_time, stop_times)\n",
    "                stop_indices = np.clip(stop_indices, 0, len(etho_time) - 1)\n",
    "                mask = (stop_indices > 0) & (np.abs(etho_time[stop_indices-1] - stop_times) < \n",
    "                                             np.abs(etho_time[stop_indices] - stop_times))\n",
    "                stop_indices[mask] -= 1\n",
    "                alldat[f][f\"{behavior_base}_stop\"] = stop_indices\n",
    "            else:\n",
    "                alldat[f][f\"{behavior_base}_stop\"] = np.array([])\n",
    "            \n",
    "            # Print summary for each behavior\n",
    "            print(f\"  {behavior_base}: {len(start_times)} start events, {len(stop_times)} stop events\")\n",
    "        \n",
    "        # Store the comprehensive unique_behaviors list (same for all mice)\n",
    "        alldat[f]['unique_behaviors'] = all_unique_behaviors\n",
    " \n",
    "    else:\n",
    "        print(f\"Mouse {alldat[f]['mouseID']}: No manual scoring data\")\n",
    "\n",
    "# Store the comprehensive list as a global variable \n",
    "unique_behaviors = all_unique_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff491ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_behaviors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b2d24",
   "metadata": {},
   "source": [
    "#### Defining time range for snips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = alldat[0]['sampling_rate']/N; # must account for downsampling w/ N, assuming sampling rate is same in all mice\n",
    "\n",
    "#use downsampled photom time and trimmed camera time\n",
    "photom_time_interval = alldat[0]['downsampled_photom_time'][1]-alldat[0]['downsampled_photom_time'][0] \n",
    "video_time_interval = np.diff(alldat[9]['trimmed_video_time']) \n",
    "avg_video_time_interval = np.mean(video_time_interval)\n",
    "print(f\"Photom time interval: {photom_time_interval}\")\n",
    "print(f\"Avg video time interval: {avg_video_time_interval}\")\n",
    "\n",
    "#time span for peri-event filtering   \n",
    "#TRANGE is range of indices ex. -100, 200 is pre_time 100 indices and post_time 200 indices\n",
    "TRANGE = [int(-1*PRE_TIME/photom_time_interval),int(POST_TIME/photom_time_interval)]\n",
    "print(f\"TRANGE: {TRANGE}\")\n",
    "idx = np.argmax(alldat[0]['downsampled_photom_time'] > TRANGE[0])\n",
    "\n",
    "#peri_time new timescale of the peri event\n",
    "peri_time = np.array(range(int(TRANGE[1] - TRANGE[0]))) / (1/photom_time_interval) - PRE_TIME\n",
    "\n",
    "print(f\"len(peri_time): {len(peri_time)}\")\n",
    "print(f\"range(photom_time snip): {alldat[0]['downsampled_photom_time'][0]} to {alldat[0]['downsampled_photom_time'][len(peri_time)-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb662a3b",
   "metadata": {},
   "source": [
    "#### Functions to map indices to photom time, pull out snips, and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find indices in photometry time that match events in etho time\n",
    "def map_etho_indices_to_photom_indices(alldat, index_array_name, output_name):\n",
    "    for entry in alldat:\n",
    "        # Check if the event exists for this mouse\n",
    "        if index_array_name not in entry:\n",
    "            # If the event doesn't exist for this mouse, create an empty array\n",
    "            entry[output_name] = np.array([])\n",
    "            continue\n",
    "        \n",
    "        indices = entry[index_array_name]\n",
    "        \n",
    "        # Check if indices array is empty\n",
    "        if len(indices) == 0:\n",
    "            entry[output_name] = np.array([])\n",
    "            continue\n",
    "        \n",
    "        vid_time = entry['trimmed_video_time']\n",
    "        photom_time = entry['downsampled_photom_time']\n",
    "\n",
    "        # Get Vid_time values at specified indices\n",
    "        event_time = vid_time[indices]\n",
    "\n",
    "        # Find the closest index in time_array for each Etho_time value\n",
    "        closest_indices = [np.argmin(np.abs(photom_time - x)) for x in event_time]\n",
    "\n",
    "        # Save to the dictionary\n",
    "        entry[output_name] = np.array(closest_indices)\n",
    "\n",
    "# Example usage\n",
    "# map_etho_indices_to_photom_indices(alldat, 'edge_entries', 'mapped_edge_entries')\n",
    "\n",
    "#Function to find indices in photometry time that match TTL timestamps\n",
    "def map_timestamps_to_photom_indices(alldat, timestamp_array_name, output_name):\n",
    "    for entry in alldat:\n",
    "        photom_time = entry['downsampled_photom_time']\n",
    "        timestamps = entry[timestamp_array_name]  # These are already timestamps, not indices\n",
    "\n",
    "        # Find the closest index in photom_time for each timestamp\n",
    "        closest_indices = [np.argmin(np.abs(photom_time - x)) for x in timestamps]\n",
    "\n",
    "        # Save to the dictionary\n",
    "        entry[output_name] = np.array(closest_indices)\n",
    "\n",
    "#Function to pull out photometry data surrounding indexed event timestamps\n",
    "def extract_snips(alldat, trange, onset_key, snips_key, mean_key, trace):\n",
    "    \"\"\"\n",
    "    Extracts snippets around event onsets for each entry in alldat.\n",
    "\n",
    "    Parameters:\n",
    "    - alldat: List of dictionaries containing data for each session\n",
    "    - trange: Tuple (start_offset, end_offset) defining snippet range relative to onset\n",
    "    - onset_key: Key in alldat[f] containing event onset indices\n",
    "    - snips_key: Key to store extracted dFF snippets\n",
    "    - mean_key: Key to store mean dFF signal across trials\n",
    "    - trace: Key for the signal trace to analyze \n",
    "\n",
    "    Modifies alldat in place by adding the extracted snippets and their mean.\n",
    "    \"\"\"\n",
    "\n",
    "    correct_size = trange[1] - trange[0]\n",
    "\n",
    "    for f in range(len(alldat)): \n",
    "        indices = alldat[f][onset_key]\n",
    "\n",
    "        trials = len(indices)\n",
    "        snips = [None] * trials\n",
    "        pre_stim = np.zeros(trials)\n",
    "        post_stim = np.zeros(trials)\n",
    "\n",
    "        for i in range(trials):\n",
    "            pre_stim[i] = max(0, (indices[i] + trange[0]))\n",
    "            post_stim[i] = min((indices[i] + trange[1]), len(alldat[f][trace])) \n",
    "\n",
    "            snips[i] = alldat[f][trace][int(pre_stim[i]):int(post_stim[i])]\n",
    "\n",
    "        # Remove NaNs and ensure correct size\n",
    "        snips = [arr for arr in snips if isinstance(arr, np.ndarray) and not np.isnan(arr).any()]\n",
    "        snips = [arr for arr in snips if len(arr) == correct_size]\n",
    "        \n",
    "        # Compute mean dFF signal\n",
    "        mean = np.mean(np.array(snips), axis=0) if snips else np.array([])\n",
    "\n",
    "        # Store results in alldat\n",
    "        alldat[f][snips_key] = snips\n",
    "        alldat[f][mean_key] = mean\n",
    "\n",
    "#Function to plot event means\n",
    "def plot_event_means(events_to_plot, suffix='_corrected_mean', peri_time=peri_time, alldat=alldat, group_ids=group_ids):\n",
    "    \"\"\"\n",
    "    Plot both individual traces and group averages for specified events.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new figure for each event\n",
    "    for event_to_plot in events_to_plot:\n",
    "        event_mean_key = f'{event_to_plot}{suffix}'\n",
    "        mice_with_no_events = []\n",
    "        has_any_data = False\n",
    "        \n",
    "        # Check for mice with no data\n",
    "        for f in range(len(alldat)):\n",
    "            mousenum = alldat[f]['mouseID']\n",
    "            \n",
    "            # Check if the data exists and has the correct shape\n",
    "            if (event_mean_key not in alldat[f] or \n",
    "                alldat[f][event_mean_key] is None or \n",
    "                len(alldat[f][event_mean_key]) == 0 or \n",
    "                len(alldat[f][event_mean_key]) != len(peri_time)):\n",
    "                mice_with_no_events.append(mousenum)\n",
    "            else:\n",
    "                has_any_data = True\n",
    "        \n",
    "        if mice_with_no_events:\n",
    "            print(f\"\\n{event_to_plot} - Mice with no events: {', '.join(map(str, mice_with_no_events))}\")\n",
    "            \n",
    "        if not has_any_data:\n",
    "            print(f\"No valid data found for event: {event_to_plot}\")\n",
    "            continue\n",
    "            \n",
    "        # Plot 1: Individual traces\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for f in range(len(alldat)): \n",
    "            mousenum = alldat[f]['mouseID']\n",
    "            \n",
    "            if (event_mean_key in alldat[f] and \n",
    "                alldat[f][event_mean_key] is not None and \n",
    "                len(alldat[f][event_mean_key]) > 0 and\n",
    "                len(alldat[f][event_mean_key]) == len(peri_time)):\n",
    "                plt.plot(peri_time, alldat[f][event_mean_key], label=f\"{mousenum} ({alldat[f]['group']})\")\n",
    "\n",
    "        plt.title(f\"{event_to_plot}{suffix}\\nIndividual Traces\")\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Signal')\n",
    "        \n",
    "        # Plot 2: Group averages\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for group_id in group_ids:\n",
    "            event_arrays = []\n",
    "\n",
    "            for f in range(len(alldat)):\n",
    "                if (alldat[f]['group'] == group_id and \n",
    "                    event_mean_key in alldat[f] and \n",
    "                    alldat[f][event_mean_key] is not None and\n",
    "                    len(alldat[f][event_mean_key]) == len(peri_time)):\n",
    "                    event_arrays.append(alldat[f][event_mean_key])\n",
    "\n",
    "            if event_arrays:  # Only process if we have data for this group\n",
    "                # Stack the arrays to compute the average and standard error\n",
    "                event_stack = np.vstack(event_arrays)\n",
    "                event_mean = np.mean(event_stack, axis=0)\n",
    "                event_sem = np.std(event_stack, axis=0) / np.sqrt(event_stack.shape[0])\n",
    "\n",
    "                # Plotting the average array with standard error bands\n",
    "                plt.plot(peri_time, event_mean, label=f'{group_id} (n={len(event_arrays)})')\n",
    "                plt.fill_between(peri_time, \n",
    "                               event_mean - event_sem, \n",
    "                               event_mean + event_sem, \n",
    "                               alpha=0.3, \n",
    "                               label='_nolegend_')\n",
    "\n",
    "        plt.title(f\"{event_to_plot}{suffix}\\nGroup Averages\")\n",
    "        plt.legend()\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Signal')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()  # Show all figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2c392",
   "metadata": {},
   "source": [
    "#### Mapping indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48dfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert etho indices to photom indices\n",
    "\n",
    "# Initialize events list\n",
    "events = []\n",
    "\n",
    "# Add velocity events if velocity data is present\n",
    "if experiment_type != 'synapse_notes_only':\n",
    "    # Check if any mouse has velocity data\n",
    "    has_velocity_data = any('velocity' in alldat[f] and len(alldat[f]['velocity']) > 0 for f in range(len(alldat)))\n",
    "    \n",
    "    if has_velocity_data:\n",
    "        # Check if velocity onsets were actually found\n",
    "        has_high_velocity = any('high_velocity_onsets' in alldat[f] and len(alldat[f]['high_velocity_onsets']) > 0 for f in range(len(alldat)))\n",
    "        has_low_velocity = any('low_velocity_onsets' in alldat[f] and len(alldat[f]['low_velocity_onsets']) > 0 for f in range(len(alldat)))\n",
    "        \n",
    "        if has_high_velocity:\n",
    "            events.append('high_velocity_onsets')\n",
    "        if has_low_velocity:\n",
    "            events.append('low_velocity_onsets')\n",
    "\n",
    "# Add zone events based on experiment configuration\n",
    "if experiment_type != 'synapse_notes_only':\n",
    "    zones = EXPERIMENT_CONFIGS[experiment_type]\n",
    "    for zone in zones:\n",
    "        zone_lower = zone.lower()\n",
    "        \n",
    "        # Check if zone entries/exits exist for any mouse\n",
    "        entries_key = f'{zone_lower}_entries'\n",
    "        exits_key = f'{zone_lower}_exits'\n",
    "        \n",
    "        has_entries = any(entries_key in alldat[f] and len(alldat[f][entries_key]) > 0 for f in range(len(alldat)))\n",
    "        has_exits = any(exits_key in alldat[f] and len(alldat[f][exits_key]) > 0 for f in range(len(alldat)))\n",
    "        \n",
    "        if has_entries:\n",
    "            events.append(entries_key)\n",
    "        if has_exits:\n",
    "            events.append(exits_key)\n",
    "\n",
    "# Add manual behaviors if they exist\n",
    "if experiment_type != 'synapse_notes_only':\n",
    "    # Check if manual behaviors were identified (from unique_behaviors)\n",
    "    if 'unique_behaviors' in locals() and len(unique_behaviors) > 0:\n",
    "        for behavior in unique_behaviors:\n",
    "            # Check if this behavior exists for any mouse\n",
    "            has_behavior = any(behavior in alldat[f] and len(alldat[f][behavior]) > 0 for f in range(len(alldat)))\n",
    "            if has_behavior:\n",
    "                events.append(behavior)\n",
    "\n",
    "# Handle synapse_notes_only separately\n",
    "if experiment_type == 'synapse_notes_only':\n",
    "    # Get only the numbered bit keys (bit1, bit2, etc.)\n",
    "    events = [key for key in alldat[0].keys() \n",
    "             if key.startswith('bit') and key[3:].isdigit()]  # ensures there's a number after 'bit'\n",
    "\n",
    "# Raise error if no events found\n",
    "if not events:\n",
    "    raise ValueError(f\"No valid events found for experiment type: {experiment_type}\")\n",
    "\n",
    "print(f\"Using events: {events}\")\n",
    "\n",
    "# Store the events list in each mouse's data for later reference\n",
    "for f in range(len(alldat)):\n",
    "    alldat[f]['events_list'] = events\n",
    "\n",
    "# Map indices based on experiment type\n",
    "if experiment_type == 'synapse_notes_only': \n",
    "    for i in range(len(events)):\n",
    "        map_timestamps_to_photom_indices(alldat, events[i], f'mapped_{events[i]}')  # for TTL timestamps\n",
    "else:\n",
    "    for i in range(len(events)):\n",
    "        map_etho_indices_to_photom_indices(alldat, events[i], f'mapped_{events[i]}')  # for etho indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e6429",
   "metadata": {},
   "source": [
    "#### Snips from motion corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get snips and means from motion corrected data\n",
    "#Note, \"corrected_snips\" here is equivalend of \"bit_snips\" in operant box code\n",
    "\n",
    "for i in range(len(events)):\n",
    "    extract_snips(alldat, TRANGE, f'mapped_{events[i]}', f'{events[i]}_corrected_snips', f'{events[i]}_corrected_mean', 'corrected_green')\n",
    "    \n",
    "    # Print summary of snips extracted for this event\n",
    "    print(f\"\\n{events[i]} snips extracted:\")\n",
    "    for f in range(len(alldat)):\n",
    "        snips_key = f'{events[i]}_corrected_snips'\n",
    "        if snips_key in alldat[f]:\n",
    "            num_snips = len(alldat[f][snips_key])\n",
    "            print(f\"  Mouse {alldat[f]['mouseID']}: {num_snips} snips\")\n",
    "        else:\n",
    "            print(f\"  Mouse {alldat[f]['mouseID']}: No snips found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd783a3",
   "metadata": {},
   "source": [
    "Plot snips from motion corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = alldat[0]['events_list'] # this will plot everything, or you can specify a list of events i.e. ['edge_entries', 'velocity'] etc.\n",
    "suffix = '_corrected_mean'  \n",
    "\n",
    "plot_event_means(events_to_plot, suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f28a2",
   "metadata": {},
   "source": [
    "#### calculate peri-event z scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60826799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_scores(alldat, fs, TRANGE, Base_start, Base_end, AUC_start, AUC_end, vel_key):\n",
    "    \"\"\"\n",
    "    Computes z-scored dF/F snippets, their means, standard errors, and AUC values.\n",
    "    \n",
    "    Parameters:\n",
    "    - alldat: List of dictionaries containing data for each session.\n",
    "    - fs: Sampling frequency.\n",
    "    - TRANGE: Time range for peri-event window.\n",
    "    - Base_start: Baseline start time (relative to event, in seconds).\n",
    "    - Base_end: Baseline end time (relative to event, in seconds).\n",
    "    - AUC_start: AUC calculation start time (relative to event, in seconds).\n",
    "    - AUC_end: AUC calculation end time (relative to event, in seconds).\n",
    "    - vel_key: Key in alldat[f] for velocity-related dF/F snippets (e.g., 'high_vel').\n",
    "\n",
    "    Modifies alldat in place by adding z-scored data, means, standard errors, and AUC values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert time parameters to indices relative to TRANGE\n",
    "    Base_start = int(Base_start * np.floor(fs) - TRANGE[0]) \n",
    "    Base_end = int(Base_end * np.floor(fs) - TRANGE[0]) \n",
    "    AUC_start = int(AUC_start * np.floor(fs) - TRANGE[0])\n",
    "    AUC_end = int(AUC_end * np.floor(fs) - TRANGE[0])\n",
    "\n",
    "    for f in range(len(alldat)):  \n",
    "        trials = len(alldat[f][f\"{vel_key}_corrected_snips\"])\n",
    "        if trials > 0:\n",
    "\n",
    "            z_snips = [None] * trials\n",
    "            z_AUC_calc = [None] * trials\n",
    "\n",
    "            for i in range(trials):\n",
    "                dFF_snip = np.array(alldat[f][f\"{vel_key}_corrected_snips\"])[i, :]\n",
    "            \n",
    "                # Compute baseline mean and std deviation\n",
    "                zb = np.mean(dFF_snip[Base_start:Base_end])  \n",
    "                zsd = np.std(dFF_snip[Base_start:Base_end])  \n",
    "\n",
    "                # Compute Z-score for each trial\n",
    "                z_snips[i] = (dFF_snip - zb) / zsd  \n",
    "\n",
    "                # Compute AUC for the specified period\n",
    "                z_AUC_calc[i] = np.trapz(z_snips[i][AUC_start:AUC_end])  \n",
    "\n",
    "            # Compute mean and standard error\n",
    "            z_means = np.mean(np.array(z_snips), axis=0)\n",
    "            z_sterr = np.std(np.array(z_snips), axis=0) / np.sqrt(np.array(z_snips).shape[0])\n",
    "\n",
    "            # Compute AUC values\n",
    "            z_AUCs = np.array(z_AUC_calc)\n",
    "            z_AUCmeans = np.mean(z_AUCs)\n",
    "\n",
    "            # Store results in alldat\n",
    "            alldat[f][f\"{vel_key}_z_snips\"] = z_snips\n",
    "            alldat[f][f\"{vel_key}_z_mean\"] = z_means\n",
    "            alldat[f][f\"{vel_key}_z_sterr\"] = z_sterr\n",
    "            alldat[f][f\"{vel_key}_z_AUCs\"] = z_AUCs\n",
    "            alldat[f][f\"{vel_key}_z_AUCmeans\"] = z_AUCmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2176cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z score for all snips\n",
    "#numbers are baseline start, baseline end, AUC start, AUC end\n",
    "for i in range(len(events)):\n",
    "    compute_z_scores(alldat, fs, TRANGE, base_start, base_end, AUC_start, AUC_end, events[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97145563",
   "metadata": {},
   "source": [
    "Plot peri-event z scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa295b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = alldat[0]['events_list'] # this will plot everything, or you can specify a list of events i.e. ['edge_entries', 'velocity'] etc.\n",
    "suffix = '_z_mean'  \n",
    "\n",
    "plot_event_means(events_to_plot, suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b71f83",
   "metadata": {},
   "source": [
    "### Z scoring to whole trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e24993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z-score to mean and stdev of whole trace\n",
    "for f in range(len(alldat)): \n",
    "    zm = np.mean(alldat[f]['corrected_green']) # mean\n",
    "    zsd = np.std(alldat[f]['corrected_green']) # stdev\n",
    "    zscore = (alldat[f]['corrected_green']-zm)/zsd\n",
    "    \n",
    "    alldat[f]['z_whole_trace']=zscore\n",
    "\n",
    "#Get snips and means for velocity and location transitions\n",
    "\n",
    "for i in range(len(events)):\n",
    "    extract_snips(alldat, TRANGE, f'mapped_{events[i]}', f'{events[i]}_z_whole_trace_snips', f'{events[i]}_z_whole_trace_mean', 'z_whole_trace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d451c",
   "metadata": {},
   "source": [
    "Plot whole trace z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_to_plot = alldat[0]['events_list']\n",
    "suffix = '_z_whole_trace_mean'  \n",
    "\n",
    "plot_event_means(events_to_plot, suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9130751",
   "metadata": {},
   "source": [
    "### Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58340b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export means\n",
    "\n",
    "# Specify which keys to export\n",
    "export_keys = ['z_mean', 'z_whole_trace_mean'] #not including corrected_mean but could be added back in\n",
    "\n",
    "triallength = len(peri_time)\n",
    "\n",
    "# Sort alldat by group and get sorted column names\n",
    "sorted_indices = sorted(range(len(alldat)), key=lambda i: alldat[i]['group'])\n",
    "colnames = [alldat[i]['mouseID'] for i in sorted_indices] \n",
    "\n",
    "for key in export_keys:\n",
    "    excel_file_path = os.path.join(data_output, f'{key}.xlsx')\n",
    "    os.makedirs(os.path.dirname(excel_file_path), exist_ok=True)\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "        for event in events:\n",
    "            event_key = f'{event}_{key}'  # construct the full key name\n",
    "            zframe = np.empty((0, triallength))\n",
    "\n",
    "            # Use sorted_indices to iterate through alldat in group order\n",
    "            for i in sorted_indices:\n",
    "                f = i  # This maintains the original index for accessing alldat[f]\n",
    "                if event_key in alldat[f]:\n",
    "                    data = alldat[f][event_key]\n",
    "                    # Check if data is empty or entirely NaN\n",
    "                    if data is not None and len(data) > 0 and np.any(~np.isnan(data)):  \n",
    "                        zframe = np.vstack((zframe, data))\n",
    "                    else:\n",
    "                        zframe = np.vstack((zframe, np.full((triallength,), np.nan)))\n",
    "                else:\n",
    "                    zframe = np.vstack((zframe, np.full((triallength,), np.nan)))\n",
    "\n",
    "            df = pd.DataFrame(zframe).T\n",
    "            df.columns = colnames\n",
    "            df.insert(0, 'peri_time', peri_time)\n",
    "            \n",
    "            # Get group labels in the same order as colnames\n",
    "            group_labels = [alldat[i]['group'] for i in sorted_indices]\n",
    "            group_row = [''] + group_labels  # Blank for peri_time column\n",
    "\n",
    "            # Insert the group row as the first row of the DataFrame\n",
    "            df_with_group = pd.concat([\n",
    "                pd.DataFrame([group_row], columns=df.columns),  # group row\n",
    "                df\n",
    "            ], ignore_index=True)\n",
    "\n",
    "            sheet_name = event[:31]  # Excel has a 31 character limit for sheet names\n",
    "            df_with_group.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 1 workbook per mouse that has all types of snips for all trials for all events\n",
    "\n",
    "# Create subfolder path\n",
    "subfolder_name = f\"individual_trials\"\n",
    "subfolder_path = os.path.join(data_output, subfolder_name)\n",
    "os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "for f in range(len(alldat)):\n",
    "    \n",
    "    excel_file_path = os.path.join(subfolder_path, f\"{alldat[f]['mouseID']}_trials.xlsx\")\n",
    "\n",
    "    with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "        for event in events:\n",
    "            # Define all possible snip types for this event\n",
    "            snip_types = {\n",
    "               # 'corrected': f'{event}_corrected_snips',\n",
    "                'z': f'{event}_z_snips',\n",
    "                'z_whole_trace': f'{event}_z_whole_trace_snips'\n",
    "            }\n",
    "            \n",
    "            # Process each snip type - create tab even if empty\n",
    "            for snip_type, key in snip_types.items():\n",
    "                # Generate sheet name: truncate event name first, then add snip_type\n",
    "                # Excel sheet name limit is 31 characters\n",
    "                # Reserve space for underscore and snip_type (longest is \"z_whole_trace\" = 13 chars)\n",
    "                max_event_len = 31 - len(snip_type) - 1  # -1 for underscore\n",
    "                truncated_event = event[:max_event_len]\n",
    "                sheet_name = f'{truncated_event}_{snip_type}'[:31]  # Final safety check\n",
    "                \n",
    "                # Check if snips exist and have data\n",
    "                if key in alldat[f] and len(alldat[f][key]) > 0:\n",
    "                    # Convert snips to DataFrame and transpose\n",
    "                    snips_df = pd.DataFrame(alldat[f][key]).T\n",
    "                    # Add time column\n",
    "                    snips_df.insert(0, 'Time', peri_time)\n",
    "                else:\n",
    "                    # Create empty DataFrame with just Time column if no data\n",
    "                    snips_df = pd.DataFrame({'Time': peri_time})\n",
    "                \n",
    "                # Save to Excel (creates tab even if empty)\n",
    "                snips_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"Individual trial workbooks created for all mice\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
